{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6beca30-c584-4bb5-a791-00d5b0b3f790",
   "metadata": {},
   "source": [
    "## Data Import ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2b3724c-cd67-4d6d-8a04-6e484316a981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331bf29f-0663-4798-bee9-9bc11e7da1d4",
   "metadata": {},
   "source": [
    "**file processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ce70121-9edc-45f6-97f0-da1850e4e3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 7000000 rows.\n"
     ]
    }
   ],
   "source": [
    "import bz2\n",
    "\n",
    "file_path = r\"C:\\Users\\katsi\\OneDrive\\Business_Analytics\\Thesis\\Data\\master-telemetry-distilled.bz2\"\n",
    "max_rows = 7_000_000  # number of rows to read\n",
    "\n",
    "row_count = 0\n",
    "\n",
    "with bz2.open(file_path, \"rt\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if row_count >= max_rows:\n",
    "            break\n",
    "\n",
    "        # Strip trailing newline; keep empty trailing fields if present\n",
    "        fields = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "\n",
    "        # Required fields\n",
    "        try:\n",
    "            timestamp, user, version, event = fields[:4]\n",
    "        except ValueError:\n",
    "            # malformed line; skip\n",
    "            continue\n",
    "\n",
    "        # Optional name field\n",
    "        name = fields[4] if len(fields) > 4 else None\n",
    "\n",
    "        # Use the values\n",
    "        # ...\n",
    "\n",
    "        row_count += 1\n",
    "\n",
    "print(f\"Processed {row_count} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcd764b-c18e-4ac0-b951-7647dbfb1fa1",
   "metadata": {},
   "source": [
    "**read & save subset of file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86c52f8e-3582-41be-8540-99bbb6fbbbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 7000000 rows into DataFrame.\n",
      "   timestamp  user   version  event command_type\n",
      "0  315522314  5129  7.2.0.50  Start         None\n",
      "1  315527925  5129  7.2.0.50  Start         None\n",
      "2  315777777  4103   7.4.0.4  Start         None\n",
      "3  315777898  4103   7.4.0.4    End         None\n",
      "4  315777899  4103   7.4.0.4  Start         None\n"
     ]
    }
   ],
   "source": [
    "import bz2\n",
    "import pandas as pd\n",
    "\n",
    "file_path = r\"C:\\Users\\katsi\\OneDrive\\Business_Analytics\\Thesis\\Data\\master-telemetry-distilled-sorted.bz2\"\n",
    "max_rows = 7_000_000  # number of rows to read\n",
    "\n",
    "rows = []  # list to store rows temporarily\n",
    "row_count = 0\n",
    "\n",
    "with bz2.open(file_path, \"rt\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if row_count >= max_rows:\n",
    "            break\n",
    "\n",
    "        # Strip trailing newline; keep empty trailing fields if present\n",
    "        fields = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "\n",
    "        # Required fields\n",
    "        try:\n",
    "            timestamp, user, version, event_full = fields[:4]\n",
    "        except ValueError:\n",
    "            # malformed line; skip\n",
    "            continue\n",
    "\n",
    "        # Optional name field\n",
    "        name = fields[4] if len(fields) > 4 else None\n",
    "\n",
    "        # Append row as a tuple/list\n",
    "        rows.append([timestamp, user, version, event_full])\n",
    "\n",
    "        row_count += 1\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(rows, columns=[\"timestamp\", \"user\", \"version\", \"event_full\"])\n",
    "\n",
    "# Split 'event_full' into 'command_type' and 'event'\n",
    "df[['event', 'command_type']] = df['event_full'].str.strip().str.split(n=1, pat=' ', expand=True)\n",
    "\n",
    "# Drop the original 'event_full' column if no longer needed\n",
    "df.drop(columns=['event_full'], inplace=True)\n",
    "\n",
    "print(f\"Processed {row_count} rows into DataFrame.\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e22aaa46-dd7a-4135-8e16-04320a7816b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000000, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61da9a22-0b6f-493a-9e0e-fb8b31af7409",
   "metadata": {},
   "source": [
    "**read, clean and save the file def**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef7ab421-a81f-49dc-a574-b72a74acd7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def preprocess_telemetry(file_path, versions_file_path):\n",
    "    # Read bz2 CSV\n",
    "    df = pd.read_csv(file_path, compression='bz2', sep='\\t', dtype=str)\n",
    "    \"\"\"\n",
    "    Clean and preprocess raw telemetry DataFrame.\n",
    "    \n",
    "    Steps:\n",
    "    1. Remove rows with missing or negative user_id\n",
    "    2. Drop duplicate rows\n",
    "    3. Keep only relevant software versions and add release_date\n",
    "    4. Create datetime, year, and month columns\n",
    "    5. Remove rows where year == 2036\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1) Remove missing or negative user_id\n",
    "    df = df[df[\"user_id\"].notna()].copy()\n",
    "    df[\"user_id\"] = pd.to_numeric(df[\"user_id\"], errors=\"coerce\")  # convert to numeric\n",
    "    df = df[df[\"user_id\"] >= 0].reset_index(drop=True)  # keep only non-negative IDs\n",
    "\n",
    "    # 2) Drop duplicate rows\n",
    "    df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # 3) Filter dataset for valid software versions\n",
    "    valid_versions = pd.read_csv(\n",
    "        versions_file_path,\n",
    "        sep='\\s+',       # splits on whitespace\n",
    "        header=None,     \n",
    "        names=['version', 'release_date']\n",
    "    )\n",
    "\n",
    "    df = df.merge(valid_versions, on='version', how='inner').reset_index(drop=True)\n",
    "\n",
    "    # 4) Create datetime column\n",
    "    df = df[df['timestamp'] != 'timestamp'].copy()  # remove header if present\n",
    "    df['timestamp'] = df['timestamp'].astype(int)\n",
    "    df['datetime'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "\n",
    "    # Create year and month columns\n",
    "    df['year'] = df['datetime'].dt.year\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "\n",
    "    # 5) Remove rows where year == 2036\n",
    "    df = df[df['year'] != 2036].reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f3d960-6af4-47c6-b707-cdc5c4ecf749",
   "metadata": {},
   "source": [
    "**Read the entire file (Slow)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d11c0331-d8eb-4ba6-9d1d-87bbeae23c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\katsi\\OneDrive\\Business_Analytics\\Thesis\\Data\\master-telemetry-distilled.bz2\"\n",
    "versions_file_path = r\"C:\\Users\\katsi\\OneDrive\\Business_Analytics\\Thesis\\Data\\Fespa & Tekton Versions.txt\"\n",
    "\n",
    "cleaned_df = preprocess_telemetry(file_path, versions_file_path)\n",
    "print(cleaned_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5139e58d-8e05-4968-877d-58ae1c45a0c0",
   "metadata": {},
   "source": [
    "**1) WindowAccumulator** (sliding window counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "32c0d8b8-21ae-45fd-90dc-978d98e431be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class WindowAccumulator:\n",
    "    def __init__(self, window_size):\n",
    "        self.window = deque()  # (timestamp, units)\n",
    "        self.window_size = window_size\n",
    "        self.total = 0\n",
    "\n",
    "    def _evict(self, now):\n",
    "        cutoff = now - self.window_size\n",
    "        while self.window and self.window[0][0] < cutoff:\n",
    "            ts, units = self.window.popleft()\n",
    "            self.total -= units\n",
    "\n",
    "    def add(self, timestamp, units):\n",
    "        self._evict(timestamp)\n",
    "        self.window.append((timestamp, units))\n",
    "        self.total += units\n",
    "\n",
    "    def get(self, timestamp):\n",
    "        self._evict(timestamp)\n",
    "        return self.total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820787ea-72a1-4a98-a7be-f056583d6ce3",
   "metadata": {},
   "source": [
    "- Purpose: keep a rolling sum of numeric units for only the events whose timestamps are within the last window_size seconds.\n",
    "\n",
    "- deque storage: events are stored as (timestamp, units) in time order (oldest at left). deque.popleft() is O(1) so eviction is cheap.\n",
    "\n",
    "- _evict(now): computes cutoff = now - window_size and removes all events strictly older than cutoff. This keeps memory bounded to events in the window.\n",
    "\n",
    "- add(timestamp, units): evicts stale events first, appends the new event, and updates total incrementally (so get() is O(1)).\n",
    "\n",
    "- get(timestamp): evicts stale events and returns total — the current sum inside the window.\n",
    "\n",
    "- Assumptions: timestamps you pass in are numeric and (ideally) non-decreasing per metric per user (amortized O(1) behavior). If timestamps jump backwards, behavior still works but may cause some extra evictions later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72ef92f-8ba5-42d3-98ed-fb23ee6ae66e",
   "metadata": {},
   "source": [
    "**2) Metric Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dfe112fc-b6fc-4be0-8b37-c3ad8d391421",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, auto\n",
    "\n",
    "class Metric(Enum):\n",
    "    COMMAND_PARAM_COUNT_WEEKLY = auto()\n",
    "    DAYS_ACTIVE_LAST_30 = auto()\n",
    "    AVG_SESSIONS_PER_WEEK = auto()\n",
    "    TOTAL_USAGE_TIME_HOURS = auto()\n",
    "    ERROR_RATE = auto()\n",
    "    # Non-rolling metrics:\n",
    "    INTER_SESSION_GAP_HOURS = auto()\n",
    "    CRASH_COUNT = auto()\n",
    "    LAST_VERSION_USED = auto()\n",
    "    COMMAND_ENTROPY = auto()\n",
    "    UNIQUE_COMMAND_COUNT = auto()\n",
    "    DAYS_SINCE_NEXT_RELEASE = auto()\n",
    "    DAYS_SINCE_LAST_USE = auto()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f667ec2-dcac-47ee-89b0-38c2494d2ec2",
   "metadata": {},
   "source": [
    "**3) Assign window sizes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "71cfdec5-4bea-45d9-888a-dab953daba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZES = {\n",
    "    Metric.COMMAND_PARAM_COUNT_WEEKLY: 7 * 86400, #weekly\n",
    "    Metric.DAYS_ACTIVE_LAST_30:        30 * 86400, #monthly\n",
    "    Metric.AVG_SESSIONS_PER_WEEK:      7 * 86400, #weekly\n",
    "    Metric.TOTAL_USAGE_TIME_HOURS:     30 * 86400, #monthly\n",
    "    Metric.ERROR_RATE:                 30 * 86400, #monthly\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c9b09e-1184-4eee-b5de-3545eb17043c",
   "metadata": {},
   "source": [
    "- Purpose: enumerate the metrics you want to compute and centrally define which metrics need rolling windows and how large those windows should be.\n",
    "\n",
    "- Why Enum: avoids string typos and makes code self-documenting (use Metric.COMMAND_PARAM_COUNT_WEEKLY not \"command_weekly\").\n",
    "\n",
    "- WINDOW_SIZES mapping: only metrics present here will get a WindowAccumulator. Metrics not in this dict are handled differently (snapshots, sets, frequency counters)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e2ee7d-3d28-494f-87f9-39cdad82d654",
   "metadata": {},
   "source": [
    "**4) Build UserStats container**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e7307096-6bd7-4eeb-b3c8-5f8f7b23293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, deque\n",
    "from math import log2\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "class UserStats:\n",
    "    def __init__(self):\n",
    "        # Rolling accumulators\n",
    "        self.roll = {\n",
    "            metric: WindowAccumulator(WINDOW_SIZES[metric])\n",
    "            for metric in WINDOW_SIZES\n",
    "        }\n",
    "\n",
    "        # Non-rolling\n",
    "        self.last_version_used = None\n",
    "        self.last_use_timestamp = None\n",
    "        self.crash_count = defaultdict(int)\n",
    "\n",
    "        # Sessions\n",
    "        self.last_start = None\n",
    "        self.last_end = None\n",
    "        self.total_gap = 0\n",
    "        self.gap_count = 0\n",
    "\n",
    "        # Command entropy (30-day window)\n",
    "        self.cmd_window = deque()\n",
    "        self.cmd_freq = defaultdict(int)\n",
    "        self.CMD_WINDOW = 30 * 86400\n",
    "\n",
    "        # Unique commands\n",
    "        self.unique_cmds = defaultdict(set)\n",
    "\n",
    "        # Active days (30-day window)\n",
    "        self.active_days = deque()\n",
    "        self.active_set = set()\n",
    "        self.ACTIVE_WINDOW = 30 * 86400\n",
    "\n",
    "#############\n",
    "## Methods ##\n",
    "#############\n",
    "\n",
    "    #evict old commands for entropy: removes commands older than 30 days from the command window.\n",
    "    def _evict_commands(self, now):\n",
    "        cutoff = now - self.CMD_WINDOW\n",
    "        while self.cmd_window and self.cmd_window[0][0] < cutoff:\n",
    "            ts, cmd = self.cmd_window.popleft()\n",
    "            self.cmd_freq[cmd] -= 1\n",
    "            if self.cmd_freq[cmd] == 0:\n",
    "                del self.cmd_freq[cmd]\n",
    "\n",
    "######################\n",
    "## DYNAMIC FEATUTES ##\n",
    "######################\n",
    "    \n",
    "    #add command event\n",
    "    def add_command(self, ts, cmd, version):\n",
    "        self._evict_commands(ts)\n",
    "        self.cmd_window.append((ts, cmd))\n",
    "        self.cmd_freq[cmd] += 1\n",
    "        self.unique_cmds[version].add(cmd)\n",
    "        self.roll[Metric.COMMAND_PARAM_COUNT_WEEKLY].add(ts, 1)\n",
    "\n",
    "    #add active day\n",
    "    def add_active_day(self, ts):\n",
    "        day = datetime.fromtimestamp(ts, tz=timezone.utc).date()\n",
    "        self.active_days.append((day, ts))\n",
    "        self.active_set.add(day)\n",
    "\n",
    "        cutoff = ts - self.ACTIVE_WINDOW\n",
    "        while self.active_days and self.active_days[0][1] < cutoff:\n",
    "            old_day, old_ts = self.active_days.popleft()\n",
    "            if old_day not in [d for d, _ in self.active_days]:\n",
    "                self.active_set.discard(old_day)\n",
    "\n",
    "        self.roll[Metric.DAYS_ACTIVE_LAST_30].add(ts, 1)\n",
    "\n",
    "    #sessions\n",
    "    def start_session(self, ts):\n",
    "        if self.last_end is not None:\n",
    "            gap = (ts - self.last_end)/3600\n",
    "            self.total_gap += gap\n",
    "            self.gap_count += 1\n",
    "        self.last_start = ts\n",
    "        self.last_use_timestamp = ts\n",
    "        self.roll[Metric.AVG_SESSIONS_PER_WEEK].add(ts, 1)\n",
    "\n",
    "    def end_session(self, ts):\n",
    "        if self.last_start is not None:\n",
    "            dur = (ts - self.last_start)/3600\n",
    "            self.roll[Metric.TOTAL_USAGE_TIME_HOURS].add(ts, dur)\n",
    "            self.last_end = ts\n",
    "            self.last_use_timestamp = ts\n",
    "            self.last_start = None\n",
    "\n",
    "    #crashes\n",
    "    def add_crash(self, ts, version):\n",
    "        self.crash_count[version] += 1\n",
    "        self.roll[Metric.ERROR_RATE].add(ts, 1)\n",
    "\n",
    "#####################\n",
    "## STATIC FEATUTES ##\n",
    "#####################\n",
    "    \n",
    "    def get(self, metric, now, version=None):\n",
    "        if metric in WINDOW_SIZES:\n",
    "            return self.roll[metric].get(now)\n",
    "\n",
    "        if metric == Metric.INTER_SESSION_GAP_HOURS:\n",
    "            return self.total_gap / self.gap_count if self.gap_count else 0\n",
    "\n",
    "        if metric == Metric.CRASH_COUNT:\n",
    "            return self.crash_count[version] if version else sum(self.crash_count.values())\n",
    "\n",
    "        if metric == Metric.LAST_VERSION_USED:\n",
    "            return self.last_version_used\n",
    "\n",
    "        if metric == Metric.UNIQUE_COMMAND_COUNT:\n",
    "            return len(self.unique_cmds.get(version, set()))\n",
    "\n",
    "        if metric == Metric.DAYS_SINCE_LAST_USE:\n",
    "            return (now - self.last_use_timestamp)/86400 if self.last_use_timestamp else None\n",
    "\n",
    "        if metric == Metric.COMMAND_ENTROPY:\n",
    "            total = sum(self.cmd_freq.values())\n",
    "            if total == 0:\n",
    "                return 0\n",
    "            return -sum((c/total)*log2(c/total) for c in self.cmd_freq.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8c0612-a781-438c-920b-96d1fb314c7b",
   "metadata": {},
   "source": [
    "**5) per user dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "932e4ec2-2442-4f2f-97f8-86ee4b9a2049",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_stats = {}\n",
    "\n",
    "def get_user(user_id):\n",
    "    stats = user_stats.get(user_id)\n",
    "    if stats is None:\n",
    "        stats = UserStats()\n",
    "        user_stats[user_id] = stats\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed4c9ff-637f-42f5-a87d-ad3995fe983c",
   "metadata": {},
   "source": [
    "- Purpose: a dictionary mapping user_id → UserStats.\n",
    "- Lazy initialization: create a UserStats only when you first see the user — simple and memory efficient for workloads with many users but many inactive ones.\n",
    "- Scalability: if you have extremely many users, consider periodic pruning of _USER_STATS for inactive users (e.g., if no events seen for 90 days), or persist older user states to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3edb30-f656-4b6c-9dbe-62e4077d0b0d",
   "metadata": {},
   "source": [
    "**6) adapt upgrade computation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927082d4-7cc2-4669-9a16-7f30857b104c",
   "metadata": {},
   "source": [
    "**read the first 5M rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b38476a-92dc-4540-99be-4dbe52adf95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000 rows, buffer size 12\n",
      "Processed 2000000 rows, buffer size 12\n",
      "Processed 3000000 rows, buffer size 12\n",
      "Processed 4000000 rows, buffer size 13\n",
      "Processed 5000000 rows, buffer size 13\n"
     ]
    }
   ],
   "source": [
    "import bz2\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# ------------------ File paths ------------------\n",
    "file_path = r\"C:\\Users\\katsi\\OneDrive\\Business_Analytics\\Thesis\\Data\\master-telemetry-distilled-sorted.bz2\"\n",
    "output_file = \"upgrade_events.csv\"\n",
    "\n",
    "valid_versions = {\n",
    "    \"5.0.0.34\",\"5.0.0.46\",\"5.1.0.2\",\"5.4.0.14\",\"5.4.0.100\",\n",
    "    \"5.6.0.6\",\"5.6.0.14\",\"7.0.0.16\",\"7.1.0.2\",\"7.1.0.28\",\n",
    "    \"7.1.0.64\",\"7.2.0.50\",\"7.4.0.4\",\"7.5.0.20\",\"7.6.0.2\",\n",
    "    \"7.6.0.4\",\"7.6.0.24\",\"7.7.0.100\",\"8.0.0.8\",\"8.1.0.2\",\n",
    "    \"8.1.0.4\",\"8.1.0.18\",\"8.1.0.22\",\"8.2.0.72\",\"8.3.0.16\",\n",
    "    \"9.1.0.16\",\"9.1.0.46\"\n",
    "}\n",
    "\n",
    "# ------------------ Version parser ------------------\n",
    "def version_to_tuple(v: str):\n",
    "    return tuple(int(x) for x in v.split(\".\") if x.isdigit())\n",
    "\n",
    "# ------------------ Initialize CSV ------------------\n",
    "FEATURES = [\n",
    "    Metric.COMMAND_PARAM_COUNT_WEEKLY,\n",
    "    Metric.DAYS_ACTIVE_LAST_30,\n",
    "    Metric.AVG_SESSIONS_PER_WEEK,\n",
    "    Metric.TOTAL_USAGE_TIME_HOURS,\n",
    "    Metric.INTER_SESSION_GAP_HOURS,\n",
    "    Metric.CRASH_COUNT,\n",
    "    Metric.LAST_VERSION_USED,\n",
    "    Metric.ERROR_RATE,\n",
    "    Metric.COMMAND_ENTROPY,\n",
    "    Metric.UNIQUE_COMMAND_COUNT,\n",
    "    Metric.DAYS_SINCE_LAST_USE\n",
    "]\n",
    "\n",
    "header_row = {'timestamp':0,'user':'','version':'','upgrade':1, **{f.name:0 for f in FEATURES}}\n",
    "pd.DataFrame([header_row]).to_csv(output_file, index=False)\n",
    "\n",
    "# ------------------ Buffer ------------------\n",
    "buffer, BUFFER_SIZE = [], 10_000\n",
    "\n",
    "def flush_buffer():\n",
    "    if buffer:\n",
    "        pd.DataFrame(buffer).to_csv(output_file, mode='a', header=False, index=False)\n",
    "        buffer.clear()\n",
    "\n",
    "def process_upgrade(ts, user_id, version, user):\n",
    "    row = {\n",
    "        'timestamp': ts,\n",
    "        'user': user_id,\n",
    "        'version': version,\n",
    "        'upgrade': 1,\n",
    "        **{f.name: user.get(f, ts, version=version) if f in (Metric.CRASH_COUNT, Metric.UNIQUE_COMMAND_COUNT) else user.get(f, ts) for f in FEATURES}\n",
    "    }\n",
    "    buffer.append(row)\n",
    "    if len(buffer) >= BUFFER_SIZE:\n",
    "        flush_buffer()\n",
    "        \n",
    "# ------------------ Main Loop ------------------\n",
    "user_stats, last_version_per_user = {}, {}\n",
    "MAX_ROWS = 5_000_000\n",
    "row_count = 0\n",
    "\n",
    "def get_user(user_id):\n",
    "    if user_id not in user_stats:\n",
    "        user_stats[user_id] = UserStats()\n",
    "    return user_stats[user_id]\n",
    "\n",
    "with bz2.open(file_path, \"rt\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        row_count += 1\n",
    "        if MAX_ROWS and row_count > MAX_ROWS:\n",
    "            break\n",
    "        if row_count % 1_000_000 == 0:\n",
    "            print(f\"Processed {row_count} rows, buffer size {len(buffer)}\")\n",
    "\n",
    "        fields = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "        if len(fields) < 4:\n",
    "            continue\n",
    "\n",
    "        ts_raw, user_id, version, event = fields[:4]\n",
    "\n",
    "        # ------------------ CLEANING ------------------\n",
    "        # (1) Validate timestamp\n",
    "        try:\n",
    "            ts = int(float(ts_raw))\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        # (2) Validate user_id\n",
    "        try:\n",
    "            user_id_int = int(user_id)\n",
    "            if user_id_int < 0:\n",
    "                continue\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        # (3) Validate version\n",
    "        if version not in valid_versions:\n",
    "            continue\n",
    "\n",
    "        # (4) Reject timestamps >= 2025\n",
    "        if ts >= 1735689600:\n",
    "            continue\n",
    "\n",
    "        # ------------------ Get user object ------------------\n",
    "        user = get_user(user_id)\n",
    "\n",
    "        # ------------------ Event handling ------------------\n",
    "        if event == \"Start\":\n",
    "            prev_version = last_version_per_user.get(user_id)\n",
    "            is_upgrade = prev_version and version_to_tuple(version) > version_to_tuple(prev_version)\n",
    "            last_version_per_user[user_id] = version\n",
    "            \n",
    "            user.start_session(ts)\n",
    "            if is_upgrade:\n",
    "                process_upgrade(ts, user_id, version, user)\n",
    "\n",
    "        elif event in (\"End\", \"Stop\", \"Exit\"):\n",
    "            user.end_session(ts)\n",
    "\n",
    "        elif event == \"Crash\":\n",
    "            user.add_crash(ts, version)\n",
    "\n",
    "        elif event.startswith((\"Command\", \"Param\")):\n",
    "            cmd_detail = event.split(\" \", 1)[1] if \" \" in event else \"UNKNOWN\"\n",
    "            user.add_command(ts, cmd_detail, version)\n",
    "\n",
    "        # ------------------ Update general stats ------------------\n",
    "        user.add_active_day(ts)\n",
    "        user.last_version_used = version\n",
    "        user.last_use_timestamp = ts\n",
    "\n",
    "# ------------------ Final flush ------------------\n",
    "flush_buffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "38a50673-d0f4-41f9-a156-4f0ed21b6d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"upgrade_events.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e11ac3c0-faef-41a1-b89d-c064a0252e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user</th>\n",
       "      <th>version</th>\n",
       "      <th>upgrade</th>\n",
       "      <th>COMMAND_PARAM_COUNT_WEEKLY</th>\n",
       "      <th>DAYS_ACTIVE_LAST_30</th>\n",
       "      <th>AVG_SESSIONS_PER_WEEK</th>\n",
       "      <th>TOTAL_USAGE_TIME_HOURS</th>\n",
       "      <th>INTER_SESSION_GAP_HOURS</th>\n",
       "      <th>CRASH_COUNT</th>\n",
       "      <th>LAST_VERSION_USED</th>\n",
       "      <th>ERROR_RATE</th>\n",
       "      <th>COMMAND_ENTROPY</th>\n",
       "      <th>UNIQUE_COMMAND_COUNT</th>\n",
       "      <th>DAYS_SINCE_LAST_USE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1009844676</td>\n",
       "      <td>5299.0</td>\n",
       "      <td>7.1.0.64</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4.444444e-03</td>\n",
       "      <td>1.737222</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0.0.16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1102094567</td>\n",
       "      <td>5299.0</td>\n",
       "      <td>7.2.0.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6406.299653</td>\n",
       "      <td>0</td>\n",
       "      <td>7.1.0.64</td>\n",
       "      <td>0</td>\n",
       "      <td>1.584963</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1136066749</td>\n",
       "      <td>5511.0</td>\n",
       "      <td>7.6.0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>6.944444e-03</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0</td>\n",
       "      <td>7.4.0.4</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1199134901</td>\n",
       "      <td>4450.0</td>\n",
       "      <td>7.6.0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.747003e-16</td>\n",
       "      <td>1530.776037</td>\n",
       "      <td>0</td>\n",
       "      <td>7.1.0.64</td>\n",
       "      <td>0</td>\n",
       "      <td>3.856365</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp    user   version  upgrade  COMMAND_PARAM_COUNT_WEEKLY  \\\n",
       "0           0     NaN       NaN        1                           0   \n",
       "1  1009844676  5299.0  7.1.0.64        1                           0   \n",
       "2  1102094567  5299.0  7.2.0.50        1                           0   \n",
       "3  1136066749  5511.0   7.6.0.4        1                           1   \n",
       "4  1199134901  4450.0   7.6.0.4        1                           0   \n",
       "\n",
       "   DAYS_ACTIVE_LAST_30  AVG_SESSIONS_PER_WEEK  TOTAL_USAGE_TIME_HOURS  \\\n",
       "0                    0                      0            0.000000e+00   \n",
       "1                    2                      2            4.444444e-03   \n",
       "2                    0                      1            0.000000e+00   \n",
       "3                    5                      3            6.944444e-03   \n",
       "4                    0                      1            3.747003e-16   \n",
       "\n",
       "   INTER_SESSION_GAP_HOURS  CRASH_COUNT LAST_VERSION_USED  ERROR_RATE  \\\n",
       "0                 0.000000            0                 0           0   \n",
       "1                 1.737222            0          7.0.0.16           0   \n",
       "2              6406.299653            0          7.1.0.64           0   \n",
       "3                 0.012500            0           7.4.0.4           0   \n",
       "4              1530.776037            0          7.1.0.64           0   \n",
       "\n",
       "   COMMAND_ENTROPY  UNIQUE_COMMAND_COUNT  DAYS_SINCE_LAST_USE  \n",
       "0         0.000000                     0                  0.0  \n",
       "1         0.000000                     0                  0.0  \n",
       "2         1.584963                     0                  0.0  \n",
       "3        -0.000000                     0                  0.0  \n",
       "4         3.856365                     0                  0.0  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7f99cad9-5fad-4d3d-9895-fc0672cec550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 15)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fe4941-e318-493a-85f0-b71edef56cc7",
   "metadata": {},
   "source": [
    "### Read 5M of Raw File ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89f3b618-cc3e-4355-8ef8-f35e3e222a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5000000 rows into DataFrame.\n",
      "   timestamp  user   version  event command_type\n",
      "0  315522314  5129  7.2.0.50  Start         None\n",
      "1  315527925  5129  7.2.0.50  Start         None\n",
      "2  315777777  4103   7.4.0.4  Start         None\n",
      "3  315777898  4103   7.4.0.4    End         None\n",
      "4  315777899  4103   7.4.0.4  Start         None\n"
     ]
    }
   ],
   "source": [
    "import bz2\n",
    "import pandas as pd\n",
    "\n",
    "file_path = r\"C:\\Users\\katsi\\OneDrive\\Business_Analytics\\Thesis\\Data\\master-telemetry-distilled-sorted.bz2\"\n",
    "max_rows = 5_000_000  # number of rows to read\n",
    "\n",
    "rows = []  # list to store rows temporarily\n",
    "row_count = 0\n",
    "\n",
    "with bz2.open(file_path, \"rt\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if row_count >= max_rows:\n",
    "            break\n",
    "\n",
    "        # Strip trailing newline; keep empty trailing fields if present\n",
    "        fields = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "\n",
    "        # Required fields\n",
    "        try:\n",
    "            timestamp, user, version, event_full = fields[:4]\n",
    "        except ValueError:\n",
    "            # malformed line; skip\n",
    "            continue\n",
    "\n",
    "        # Optional name field\n",
    "        name = fields[4] if len(fields) > 4 else None\n",
    "\n",
    "        # Append row as a tuple/list\n",
    "        rows.append([timestamp, user, version, event_full])\n",
    "\n",
    "        row_count += 1\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(rows, columns=[\"timestamp\", \"user\", \"version\", \"event_full\"])\n",
    "\n",
    "# Split 'event_full' into 'command_type' and 'event'\n",
    "df[['event', 'command_type']] = df['event_full'].str.strip().str.split(n=1, pat=' ', expand=True)\n",
    "\n",
    "# Drop the original 'event_full' column if no longer needed\n",
    "df.drop(columns=['event_full'], inplace=True)\n",
    "\n",
    "print(f\"Processed {row_count} rows into DataFrame.\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448cb16b-982e-4140-8772-1da852a57515",
   "metadata": {},
   "source": [
    "**1) Handle Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4a51687-b136-48c8-942f-72c1ca33fed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp            0\n",
       "user                 0\n",
       "version              0\n",
       "event                0\n",
       "command_type    107100\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6e75be-590d-445b-8c05-bd50cbe62850",
   "metadata": {},
   "source": [
    "**2) Remove rows where user_id is missing or is negative**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16819832-9daf-4710-a166-5a3a1a14a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"user\"] = pd.to_numeric(df[\"user\"], errors=\"coerce\")\n",
    "df1 = df[df[\"user\"].notna() & (df[\"user\"] >= 0)]\n",
    "df1 = df1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d53350-ad60-47bd-b9f1-5dd557e48b3f",
   "metadata": {},
   "source": [
    "**3) Handle duplicate rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47468a4b-5c10-4252-8e4f-967dfbd02faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156513"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count duplicate rows\n",
    "df1.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5394aed3-8f2f-4ea5-b4af-4e09bbe30607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicate rows\n",
    "df2 = df1.drop_duplicates().copy()\n",
    "\n",
    "#reset index\n",
    "df2 = df2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5499e1bc-9aaa-48bc-a1db-09fccdb3a05f",
   "metadata": {},
   "source": [
    "**4) Filtering Dataset by Relevant Software Versions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b1d9447-9ca2-4e02-825d-74e7c87fd759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   timestamp  user   version  event command_type\n",
      "0  315522314  5129  7.2.0.50  Start         None\n",
      "1  315527925  5129  7.2.0.50  Start         None\n",
      "2  315777777  4103   7.4.0.4  Start         None\n",
      "3  315777898  4103   7.4.0.4    End         None\n",
      "4  315777899  4103   7.4.0.4  Start         None\n"
     ]
    }
   ],
   "source": [
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e299fb1-45b5-4ad4-be0f-f5cf25a8fff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.copy()\n",
    "\n",
    "# Read valid versions and their release dates from the txt file\n",
    "valid_versions = pd.read_csv(\n",
    "    'C:/Users/katsi/OneDrive/Business_Analytics/Thesis/Data/Fespa & Tekton Versions.txt', \n",
    "    sep='\\s+',       # splits on whitespace\n",
    "    header=None,     # no header in txt\n",
    "    names=['version', 'release_date']\n",
    ")\n",
    "\n",
    "# keep only valid versions and add release_date\n",
    "df3 = df3.merge(valid_versions, on='version', how='inner')\n",
    "\n",
    "# Reset index correctly\n",
    "df3.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7892d6ae-b5da-47dd-9867-d389df40948b",
   "metadata": {},
   "source": [
    "**5) Remove row where year is equal to '2036'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e06a7af-a18e-4599-9b2c-167c7f0498bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamps to numeric (invalid ones become NaN)\n",
    "df3['timestamp'] = pd.to_numeric(df3['timestamp'], errors='coerce')\n",
    "\n",
    "# Remove rows where timestamp is missing or from 2025+\n",
    "df4 = df3[df3['timestamp'] < 1735689600].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dde9918-99e7-457f-bd5f-dd47be38bf31",
   "metadata": {},
   "source": [
    "**6) Compute Upgrade Variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97e10994-ebe9-45b7-b874-0e09ae1d436d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp        int64\n",
       "user             int64\n",
       "version         object\n",
       "event           object\n",
       "command_type    object\n",
       "release_date    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "faaf8cb5-0718-4f37-9439-225fc3d74034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a working copy\n",
    "df5 = df4.copy()\n",
    "\n",
    "# Ensure version is string\n",
    "df5['version'] = df5['version'].astype(str)\n",
    "\n",
    "# Convert version string to tuple\n",
    "def version_to_tuple(v):\n",
    "    return tuple(map(int, v.split('.')))\n",
    "df5['version_tuple'] = df5['version'].apply(version_to_tuple)\n",
    "\n",
    "# Sort by user and timestamp\n",
    "df5 = df5.sort_values(['user_id', 'timestamp']).reset_index(drop=True)\n",
    "\n",
    "# Initialize upgrade column\n",
    "df5['upgrade'] = 0\n",
    "\n",
    "\n",
    "def mark_first_upgrades(group):\n",
    "    \"\"\"\n",
    "    For each user:\n",
    "    - Identify only the FIRST Start event of each version\n",
    "    - Mark upgrade = 1 ONLY if that version is newer than any previous version\n",
    "    \"\"\"\n",
    "\n",
    "    starts = group[group['event'] == 'Start'].copy()\n",
    "    if starts.empty:\n",
    "        return pd.Series(0, index=group.index)\n",
    "\n",
    "    # Keep only the first Start per version\n",
    "    first_starts = (\n",
    "        starts.sort_values('timestamp')\n",
    "              .groupby('version_tuple')\n",
    "              .head(1)\n",
    "              .copy()\n",
    "    )\n",
    "\n",
    "    # Determine upgrade: version > previous max_version\n",
    "    first_starts = first_starts.sort_values('timestamp')\n",
    "    prev_version = (-1, -1, -1)  # always smaller than any real version\n",
    "\n",
    "    upgrade_flags = []\n",
    "\n",
    "    for idx, row in first_starts.iterrows():\n",
    "        if row['version_tuple'] > prev_version:\n",
    "            upgrade_flags.append((idx, 1))\n",
    "            prev_version = row['version_tuple']\n",
    "        else:\n",
    "            upgrade_flags.append((idx, 0))\n",
    "\n",
    "    # Create mapping back to original DataFrame\n",
    "    upgrade_series = pd.Series(0, index=group.index)\n",
    "    for idx, flag in upgrade_flags:\n",
    "        upgrade_series.loc[idx] = flag\n",
    "\n",
    "    return upgrade_series\n",
    "\n",
    "\n",
    "# Apply per user\n",
    "df5['upgrade'] = df5.groupby('user').apply(mark_first_upgrades).reset_index(level=0, drop=True)\n",
    "\n",
    "# Remove temporary column\n",
    "df5.drop(columns='version_tuple', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feda942-13e6-4ec4-a680-5f72d02ffd8c",
   "metadata": {},
   "source": [
    "**verification steps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c463d385-ea1b-4873-a170-b40e7d3c541c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "729"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5['upgrade'].sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Thesis)",
   "language": "python",
   "name": "venv_thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
